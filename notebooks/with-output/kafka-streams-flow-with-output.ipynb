{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafka-streams-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells needed to run your application are included below. Make any changes and add your sources, analytics and outputs.\n",
    "\n",
    "### Documentation\n",
    "   - [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "   - [Streams Python API](https://streamsxtopology.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install  python packages\n",
    "Installs the required python packages with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil==2.8.0 in /home/wsuser/.local/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /user-home/_global_/python-3 (from python-dateutil==2.8.0) (1.12.0)\n",
      "Requirement already satisfied: streamsx==1.14.13 in /user-home/_global_/python-3 (1.14.13)\n",
      "Requirement already satisfied: future in /user-home/_global_/python-3 (from streamsx==1.14.13) (0.17.1)\n",
      "Requirement already satisfied: dill<0.3.1,>=0.2.8.2 in /user-home/_global_/python-3 (from streamsx==1.14.13) (0.2.9)\n",
      "Requirement already satisfied: requests in /user-home/_global_/python-3 (from streamsx==1.14.13) (2.21.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user python-dateutil==2.8.0\n",
    "!pip install --user streamsx.kafka>=1.9.0\n",
    "!pip install --user streamsx==1.14.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Sets up the Streams instance name and extracts the resources required for the Streams application to a local directory.\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "To change the instance for the Streams application:\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of streams_instance_name in the cell below according to your Streams instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_lib import Project\n",
    "import os, shutil, tarfile\n",
    "from icpd_core import icpd_util    \n",
    "\n",
    "def setup(archive, resource_path):\n",
    "    def extract_project_file(file, path):\n",
    "        project = Project.access()\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        buffio = project.get_file(file, direct_storage=True)\n",
    "        tarfile.open(fileobj=buffio, mode=\"r:gz\").extractall(path)\n",
    "    extract_project_file(archive, resource_path)\n",
    "    os.chdir(resource_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_instance_name = \"streams\"\n",
    "cfg = icpd_util.get_service_instance_details(streams_instance_name)\n",
    "resource_path = \"streams_flows_notebooks/kafka_streams_flow_1597176040664\"\n",
    "setup(\"streams_flows_notebooks/kafka_streams_flow_1597176040664.tar.gz\", resource_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing flow_schemas\n"
     ]
    }
   ],
   "source": [
    "%%writefile flow_schemas\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "DEFAULT_DATETIME = datetime.fromtimestamp(0)\n",
    "\n",
    "\n",
    "class SampleDataSchema(NamedTuple):\n",
    "    click_event_type: str = \"\"\n",
    "    customer_id: float = 0.0\n",
    "    time_stamp: datetime = DEFAULT_DATETIME\n",
    "    total_number_of_distinct_items_in_basket: float = 0.0\n",
    "    total_number_of_items_in_basket: float = 0.0\n",
    "    total_price_of_basket: float = 0.0\n",
    "    product_category: str = \"\"\n",
    "    product_name: str = \"\"\n",
    "    product_price: float = 0.0\n",
    "    session_duration: float = 0.0\n",
    "\n",
    "\n",
    "class JsonFromTuple1Schema(NamedTuple):\n",
    "    content: str = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from streamsx.topology.topology import Topology\n",
    "import flow_schemas\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "from lib.error_utils import TupleError\n",
    "import lib.file_utils as file_utils\n",
    "from lib.sampledata.sample_data_producer import SampleDataProducer\n",
    "from lib.type_adapter import adapt_if_needed as _adapt_if_needed\n",
    "import os\n",
    "import streamsx.kafka as kafka\n",
    "import time\n",
    "import typing\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# MAIN\n",
    "\n",
    "def build_flow():\n",
    "    topo = Topology(name='kafka_streams_flow', namespace=os.environ.get('USER', 'flow'))\n",
    "    topo.name_to_runtime_id = name_mapping().get\n",
    "\n",
    "    topo.add_pip_package('python-dateutil==2.8.0')\n",
    "    topo.add_pip_package('streamsx.kafka>=1.9.0')\n",
    "\n",
    "    sample_data_stream = add_sample_data(topo)  # Node: \"Sample Data\"\n",
    "    kafka_stream = add_kafka(sample_data_stream)  # Node: \"Kafka\"\n",
    "\n",
    "    add_views(topo)\n",
    "    return topo\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Function for top-level operator: Sample Data\n",
    "def add_sample_data(topo):\n",
    "    return (\n",
    "        topo\n",
    "        .source(\n",
    "            generate_sample_data,\n",
    "            name='Sample Data')\n",
    "        .filter(\n",
    "            lambda event: True,\n",
    "            name='CompositeOutput1')\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Function for top-level operator: Kafka\n",
    "def add_kafka(stream):\n",
    "    connection = file_utils.read_from_json(os.path.abspath(\"connections/kafka_4560768a-c25f-49e7-9333-23726b8ae71e.json\"))\n",
    "\n",
    "    return (\n",
    "        stream\n",
    "        .map(\n",
    "            lambda event: {\n",
    "                'content':\n",
    "                    json.dumps({\n",
    "                        'click_event_type': event.click_event_type,\n",
    "                        'customer_id': event.customer_id,\n",
    "                        'time_stamp': event.time_stamp.isoformat(),\n",
    "                        'total_number_of_distinct_items_in_basket': event.total_number_of_distinct_items_in_basket,\n",
    "                        'total_number_of_items_in_basket': event.total_number_of_items_in_basket,\n",
    "                        'total_price_of_basket': event.total_price_of_basket,\n",
    "                        'product_category': event.product_category,\n",
    "                        'product_name': event.product_name,\n",
    "                        'product_price': event.product_price,\n",
    "                        'session_duration': event.session_duration\n",
    "                    })\n",
    "            },\n",
    "            name='JsonFromTuple1',\n",
    "            schema=flow_schemas.JsonFromTuple1Schema)\n",
    "        .for_each(\n",
    "            kafka.KafkaProducer(\n",
    "                config={\n",
    "                    'bootstrap.servers': connection['brokers'],\n",
    "                    'security.protocol': connection['security_protocol'],\n",
    "                    'sasl.mechanism': connection['sasl_mechanism'],\n",
    "                    'sasl.jaas.config': f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{connection[\"username\"]}\" password=\"{connection[\"api_key\"]}\";'\n",
    "                },\n",
    "                message_attribute_name='content',\n",
    "                topic='clicks'),\n",
    "            name='Kafka')\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Operator-specific global code, such as filter classes:\n",
    "\n",
    "def generate_sample_data() -> typing.Iterable[flow_schemas.SampleDataSchema]:\n",
    "    producer = SampleDataProducer(['clickStreamSampleData'])\n",
    "    offset = 0\n",
    "    while True:\n",
    "        sample_events = producer.get_events('clickStreamSampleData', offset, 1)\n",
    "        sample_event = sample_events[0]\n",
    "        offset += 1\n",
    "\n",
    "        errors = []\n",
    "        try:\n",
    "            output_event = flow_schemas.SampleDataSchema(\n",
    "                click_event_type=_adapt_if_needed(sample_event.click_event_type, str, 'click_event_type', 'Text', errors),\n",
    "                customer_id=_adapt_if_needed(sample_event.customer_id, float, 'customer_id', 'Number', errors),\n",
    "                time_stamp=_adapt_if_needed(sample_event.time_stamp, datetime.datetime, 'time_stamp', 'Date', errors),\n",
    "                total_number_of_distinct_items_in_basket=_adapt_if_needed(sample_event.total_number_of_distinct_items_in_basket, float, 'total_number_of_distinct_items_in_basket', 'Number', errors),\n",
    "                total_number_of_items_in_basket=_adapt_if_needed(sample_event.total_number_of_items_in_basket, float, 'total_number_of_items_in_basket', 'Number', errors),\n",
    "                total_price_of_basket=_adapt_if_needed(sample_event.total_price_of_basket, float, 'total_price_of_basket', 'Number', errors),\n",
    "                product_category=_adapt_if_needed(sample_event.product_category, str, 'product_category', 'Text', errors),\n",
    "                product_name=_adapt_if_needed(sample_event.product_name, str, 'product_name', 'Text', errors),\n",
    "                product_price=_adapt_if_needed(sample_event.product_price, float, 'product_price', 'Number', errors),\n",
    "                session_duration=_adapt_if_needed(sample_event.session_duration, float, 'session_duration', 'Number', errors)\n",
    "            )\n",
    "            if len(errors) > 0:\n",
    "                raise ValueError('\\n'.join(errors))\n",
    "            yield output_event\n",
    "        except Exception as err:\n",
    "            TupleError(operation_id='Sample Data', message=str(err))\n",
    "\n",
    "        time.sleep(0.05)\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Utils:\n",
    "\n",
    "def add_views(topo):\n",
    "    name_to_id = name_mapping()\n",
    "    for name, stream in topo.streams.items():\n",
    "        stream_id = name_to_id.get(name)\n",
    "        if stream_id and stream_id.endswith('__Composite_Output_Id'):\n",
    "            stream.view(name=stream_id + \"__output\")\n",
    "\n",
    "\n",
    "def name_mapping():\n",
    "    return {\n",
    "        'Sample Data': 'Sample_Data',\n",
    "        'CompositeOutput1': 'Sample_Data__Composite_Output_Id',\n",
    "        'JsonFromTuple1': 'JsonFromTuple1',\n",
    "        'Kafka': 'Kafka'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties file /tmp/wsuser/producer-40jfnu_b.properties generated.\n",
      "Properties file etc/producer-40jfnu_b.properties added to the topology kafka_streams_flow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e12c01acad4d0b92dfdca16c0624dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-13 22:13:27,430 streamsx.topology.context [INFO] Generating SPL and submitting application.\n",
      "2020-08-13 22:13:27,950 streamsx.topology.context [WARNING] Insecure host connections enabled.\n",
      "2020-08-13 22:13:29,478 streamsx.topology.context [WARNING] Insecure host connections enabled.\n",
      "2020-08-13 22:14:29,324 streamsx.topology.context [WARNING] Insecure host connections enabled.\n",
      "/user-home/_global_/python-3/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId:  20 \n",
      "Job name:  flow:kafka_streams_flow:2020-08-13_22:13:27\n"
     ]
    }
   ],
   "source": [
    "import streamsx\n",
    "import datetime\n",
    "from streamsx.topology.context import ContextTypes, JobConfig\n",
    "from streamsx.topology import context\n",
    "\n",
    "def submit_app():\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    app = build_flow()\n",
    "\n",
    "    dt = datetime.datetime.now().strftime('%F_%T')\n",
    "    \n",
    "    job_config = JobConfig(job_name=f'{app.namespace}:{app.name}:{dt}', tracing='info')\n",
    "    job_config.add(cfg)\n",
    "\n",
    "    shutil.copytree('lib', 'python/modules/lib')\n",
    "    app.add_file_dependency('python', 'opt')\n",
    "\n",
    "    submission_result = streamsx.topology.context.submit(ContextTypes.DISTRIBUTED, app, config=cfg)\n",
    "    streams_job = submission_result.job\n",
    "    print(\"JobId: \", streams_job.id, \"\\nJob name: \", streams_job.name)\n",
    "submit_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the resource directory (Optional)\n",
    "Cleans up the resource folders used in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup()\n",
    "# import shutil\n",
    "# os.chdir(os.environ['PWD'])\n",
    "# if os.path.exists(resource_path):\n",
    "#     shutil.rmtree(resource_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
