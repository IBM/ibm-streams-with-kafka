{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafka-streams-flows-as-source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells needed to run your application are included below. Make any changes and add your sources, analytics and outputs.\n",
    "\n",
    "### Documentation\n",
    "   - [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "   - [Streams Python API](https://streamsxtopology.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install  python packages\n",
    "Installs the required python packages with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamsx==1.14.13 in /user-home/_global_/python-3 (1.14.13)\n",
      "Requirement already satisfied: dill<0.3.1,>=0.2.8.2 in /user-home/_global_/python-3 (from streamsx==1.14.13) (0.2.9)\n",
      "Requirement already satisfied: requests in /user-home/_global_/python-3 (from streamsx==1.14.13) (2.21.0)\n",
      "Requirement already satisfied: future in /user-home/_global_/python-3 (from streamsx==1.14.13) (0.17.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /user-home/_global_/python-3 (from requests->streamsx==1.14.13) (2019.3.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user streamsx.kafka>=1.9.0\n",
    "!pip install --user streamsx==1.14.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Sets up the Streams instance name and extracts the resources required for the Streams application to a local directory.\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "To change the instance for the Streams application:\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of streams_instance_name in the cell below according to your Streams instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_lib import Project\n",
    "import os, shutil, tarfile\n",
    "from icpd_core import icpd_util    \n",
    "\n",
    "def setup(archive, resource_path):\n",
    "    def extract_project_file(file, path):\n",
    "        project = Project.access()\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        buffio = project.get_file(file, direct_storage=True)\n",
    "        tarfile.open(fileobj=buffio, mode=\"r:gz\").extractall(path)\n",
    "    extract_project_file(archive, resource_path)\n",
    "    os.chdir(resource_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_instance_name = \"streams\"\n",
    "cfg = icpd_util.get_service_instance_details(streams_instance_name)\n",
    "resource_path = \"streams_flows_notebooks/kafka_streams_flows_as_source_1597265335685\"\n",
    "setup(\"streams_flows_notebooks/kafka_streams_flows_as_source_1597265335685.tar.gz\", resource_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing flow_schemas\n"
     ]
    }
   ],
   "source": [
    "%%writefile flow_schemas\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class KafkaSchema(NamedTuple):\n",
    "    event_key: str = \"\"\n",
    "    event_topic: str = \"\"\n",
    "    event_offset: int = 0.0\n",
    "    event_partition: int = 0.0\n",
    "    event_timestamp: int = 0.0\n",
    "    event_message: str = \"\"\n",
    "\n",
    "\n",
    "class SchemaMapper1Schema(NamedTuple):\n",
    "    key: str = \"\"\n",
    "    topic: str = \"\"\n",
    "    offset: str = \"\"\n",
    "    partition: float = 0.0\n",
    "    time: str = \"\"\n",
    "    message: str = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from streamsx.topology.topology import Topology\n",
    "import flow_schemas\n",
    "\n",
    "from lib.error_utils import TupleError\n",
    "import lib.file_utils as file_utils\n",
    "import os\n",
    "import streamsx.kafka as kafka\n",
    "import typing\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# MAIN\n",
    "\n",
    "def build_flow():\n",
    "    topo = Topology(name='kafka_streams_flows_as_source', namespace=os.environ.get('USER', 'flow'))\n",
    "    topo.name_to_runtime_id = name_mapping().get\n",
    "\n",
    "    topo.add_pip_package('streamsx.kafka>=1.9.0')\n",
    "\n",
    "    kafka_stream = add_kafka(topo)  # Node: \"Kafka\"\n",
    "    debug_stream = add_debug(kafka_stream)  # Node: \"Debug\"\n",
    "\n",
    "    add_views(topo)\n",
    "    return topo\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Function for top-level operator: Kafka\n",
    "def add_kafka(topo):\n",
    "    connection = file_utils.read_from_json(os.path.abspath(\"connections/kafka_4560768a-c25f-49e7-9333-23726b8ae71e.json\"))\n",
    "\n",
    "    return (\n",
    "        topo\n",
    "        .source(\n",
    "            kafka.KafkaConsumer(\n",
    "                config={\n",
    "                    'bootstrap.servers': connection['brokers'],\n",
    "                    'security.protocol': connection['security_protocol'],\n",
    "                    'sasl.mechanism': connection['sasl_mechanism'],\n",
    "                    'sasl.jaas.config': f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{connection[\"username\"]}\" password=\"{connection[\"api_key\"]}\";',\n",
    "                    'auto.offset.reset': 'latest'\n",
    "                },\n",
    "                topic=\"clicks\",\n",
    "                message_attribute_name='event_message',\n",
    "                key_attribute_name='event_key',\n",
    "                topic_attribute_name='event_topic',\n",
    "                offset_attribute_name='event_offset',\n",
    "                partition_attribute_name='event_partition',\n",
    "                timestamp_attribute_name='event_timestamp',\n",
    "                schema=flow_schemas.KafkaSchema),\n",
    "            name='Kafka')\n",
    "        .map(\n",
    "            _map_schema_for_kafka,\n",
    "            name='SchemaMapper1',\n",
    "            schema=flow_schemas.SchemaMapper1Schema)\n",
    "        .filter(\n",
    "            lambda event: True,\n",
    "            name='CompositeOutput1')\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Function for top-level operator: Debug\n",
    "def add_debug(stream):\n",
    "    return (\n",
    "        stream\n",
    "        .for_each(\n",
    "            debug,\n",
    "            name='Debug')\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Operator-specific global code, such as filter classes:\n",
    "\n",
    "def _map_schema_for_kafka(event):\n",
    "    try:\n",
    "        return flow_schemas.SchemaMapper1Schema(\n",
    "            key=event.event_key,\n",
    "            topic=event.event_topic,\n",
    "            offset=str(event.event_offset),\n",
    "            partition=float(event.event_partition),\n",
    "            time=str(event.event_timestamp),\n",
    "            message=event.event_message\n",
    "        )\n",
    "    except Exception as err:\n",
    "        TupleError(operation_id='Kafka', message=str(err))\n",
    "        return None\n",
    "\n",
    "\n",
    "def debug(event):\n",
    "    # you can add debugging/logging code here\n",
    "    pass\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Utils:\n",
    "\n",
    "def add_views(topo):\n",
    "    name_to_id = name_mapping()\n",
    "    for name, stream in topo.streams.items():\n",
    "        stream_id = name_to_id.get(name)\n",
    "        if stream_id and stream_id.endswith('__Composite_Output_Id'):\n",
    "            stream.view(name=stream_id + \"__output\")\n",
    "\n",
    "\n",
    "def name_mapping():\n",
    "    return {\n",
    "        'Kafka': 'Kafka',\n",
    "        'SchemaMapper1': 'SchemaMapper1',\n",
    "        'CompositeOutput1': 'Kafka__Composite_Output_Id',\n",
    "        'Debug': 'Debug'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties file /tmp/wsuser/consumer-5yyttcd9.properties generated.\n",
      "Properties file etc/consumer-5yyttcd9.properties added to the topology kafka_streams_flows_as_source\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daba4aeb9eb4018a0b91d1e5c93eb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-12 20:49:58,499 streamsx.topology.context [INFO] Generating SPL and submitting application.\n",
      "2020-08-12 20:49:59,041 streamsx.topology.context [WARNING] Insecure host connections enabled.\n",
      "2020-08-12 20:50:00,633 streamsx.topology.context [WARNING] Insecure host connections enabled.\n",
      "2020-08-12 20:50:54,528 streamsx.topology.context [WARNING] Insecure host connections enabled.\n",
      "/user-home/_global_/python-3/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId:  18 \n",
      "Job name:  flow:kafka_streams_flows_as_source:2020-08-12_20:49:58\n"
     ]
    }
   ],
   "source": [
    "import streamsx\n",
    "import datetime\n",
    "from streamsx.topology.context import ContextTypes, JobConfig\n",
    "from streamsx.topology import context\n",
    "\n",
    "def submit_app():\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    app = build_flow()\n",
    "\n",
    "    dt = datetime.datetime.now().strftime('%F_%T')\n",
    "    \n",
    "    job_config = JobConfig(job_name=f'{app.namespace}:{app.name}:{dt}', tracing='info')\n",
    "    job_config.add(cfg)\n",
    "\n",
    "    shutil.copytree('lib', 'python/modules/lib')\n",
    "    app.add_file_dependency('python', 'opt')\n",
    "\n",
    "    submission_result = streamsx.topology.context.submit(ContextTypes.DISTRIBUTED, app, config=cfg)\n",
    "    streams_job = submission_result.job\n",
    "    print(\"JobId: \", streams_job.id, \"\\nJob name: \", streams_job.name)\n",
    "submit_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the resource directory (Optional)\n",
    "Cleans up the resource folders used in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup()\n",
    "# import shutil\n",
    "# os.chdir(os.environ['PWD'])\n",
    "# if os.path.exists(resource_path):\n",
    "#     shutil.rmtree(resource_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
