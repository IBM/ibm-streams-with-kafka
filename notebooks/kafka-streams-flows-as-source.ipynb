{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafka-streams-flows-as-source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells needed to run your application are included below. Make any changes and add your sources, analytics and outputs.\n",
    "\n",
    "### Documentation\n",
    "   - [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "   - [Streams Python API](https://streamsxtopology.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install  python packages\n",
    "Installs the required python packages with pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user streamsx.kafka>=1.9.0\n",
    "!pip install --user streamsx==1.14.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Sets up the Streams instance name and extracts the resources required for the Streams application to a local directory.\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "To change the instance for the Streams application:\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of streams_instance_name in the cell below according to your Streams instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_lib import Project\n",
    "import os, shutil, tarfile\n",
    "from icpd_core import icpd_util    \n",
    "\n",
    "def setup(archive, resource_path):\n",
    "    def extract_project_file(file, path):\n",
    "        project = Project.access()\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        buffio = project.get_file(file, direct_storage=True)\n",
    "        tarfile.open(fileobj=buffio, mode=\"r:gz\").extractall(path)\n",
    "    extract_project_file(archive, resource_path)\n",
    "    os.chdir(resource_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_instance_name = \"streams\"\n",
    "cfg = icpd_util.get_service_instance_details(streams_instance_name)\n",
    "resource_path = \"streams_flows_notebooks/kafka_streams_flows_as_source_1597265335685\"\n",
    "setup(\"streams_flows_notebooks/kafka_streams_flows_as_source_1597265335685.tar.gz\", resource_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flow_schemas\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class KafkaSchema(NamedTuple):\n",
    "    event_key: str = \"\"\n",
    "    event_topic: str = \"\"\n",
    "    event_offset: int = 0.0\n",
    "    event_partition: int = 0.0\n",
    "    event_timestamp: int = 0.0\n",
    "    event_message: str = \"\"\n",
    "\n",
    "\n",
    "class SchemaMapper1Schema(NamedTuple):\n",
    "    key: str = \"\"\n",
    "    topic: str = \"\"\n",
    "    offset: str = \"\"\n",
    "    partition: float = 0.0\n",
    "    time: str = \"\"\n",
    "    message: str = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from streamsx.topology.topology import Topology\n",
    "import flow_schemas\n",
    "\n",
    "from lib.error_utils import TupleError\n",
    "import lib.file_utils as file_utils\n",
    "import os\n",
    "import streamsx.kafka as kafka\n",
    "import typing\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# MAIN\n",
    "\n",
    "def build_flow():\n",
    "    topo = Topology(name='kafka_streams_flows_as_source', namespace=os.environ.get('USER', 'flow'))\n",
    "    topo.name_to_runtime_id = name_mapping().get\n",
    "\n",
    "    topo.add_pip_package('streamsx.kafka>=1.9.0')\n",
    "\n",
    "    kafka_stream = add_kafka(topo)  # Node: \"Kafka\"\n",
    "    debug_stream = add_debug(kafka_stream)  # Node: \"Debug\"\n",
    "\n",
    "    add_views(topo)\n",
    "    return topo\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Function for top-level operator: Kafka\n",
    "def add_kafka(topo):\n",
    "    connection = file_utils.read_from_json(os.path.abspath(\"connections/kafka_4560768a-c25f-49e7-9333-23726b8ae71e.json\"))\n",
    "\n",
    "    return (\n",
    "        topo\n",
    "        .source(\n",
    "            kafka.KafkaConsumer(\n",
    "                config={\n",
    "                    'bootstrap.servers': connection['brokers'],\n",
    "                    'security.protocol': connection['security_protocol'],\n",
    "                    'sasl.mechanism': connection['sasl_mechanism'],\n",
    "                    'sasl.jaas.config': f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{connection[\"username\"]}\" password=\"{connection[\"api_key\"]}\";',\n",
    "                    'auto.offset.reset': 'latest'\n",
    "                },\n",
    "                topic=\"clicks\",\n",
    "                message_attribute_name='event_message',\n",
    "                key_attribute_name='event_key',\n",
    "                topic_attribute_name='event_topic',\n",
    "                offset_attribute_name='event_offset',\n",
    "                partition_attribute_name='event_partition',\n",
    "                timestamp_attribute_name='event_timestamp',\n",
    "                schema=flow_schemas.KafkaSchema),\n",
    "            name='Kafka')\n",
    "        .map(\n",
    "            _map_schema_for_kafka,\n",
    "            name='SchemaMapper1',\n",
    "            schema=flow_schemas.SchemaMapper1Schema)\n",
    "        .filter(\n",
    "            lambda event: True,\n",
    "            name='CompositeOutput1')\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Function for top-level operator: Debug\n",
    "def add_debug(stream):\n",
    "    return (\n",
    "        stream\n",
    "        .for_each(\n",
    "            debug,\n",
    "            name='Debug')\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Operator-specific global code, such as filter classes:\n",
    "\n",
    "def _map_schema_for_kafka(event):\n",
    "    try:\n",
    "        return flow_schemas.SchemaMapper1Schema(\n",
    "            key=event.event_key,\n",
    "            topic=event.event_topic,\n",
    "            offset=str(event.event_offset),\n",
    "            partition=float(event.event_partition),\n",
    "            time=str(event.event_timestamp),\n",
    "            message=event.event_message\n",
    "        )\n",
    "    except Exception as err:\n",
    "        TupleError(operation_id='Kafka', message=str(err))\n",
    "        return None\n",
    "\n",
    "\n",
    "def debug(event):\n",
    "    # you can add debugging/logging code here\n",
    "    pass\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "# Utils:\n",
    "\n",
    "def add_views(topo):\n",
    "    name_to_id = name_mapping()\n",
    "    for name, stream in topo.streams.items():\n",
    "        stream_id = name_to_id.get(name)\n",
    "        if stream_id and stream_id.endswith('__Composite_Output_Id'):\n",
    "            stream.view(name=stream_id + \"__output\")\n",
    "\n",
    "\n",
    "def name_mapping():\n",
    "    return {\n",
    "        'Kafka': 'Kafka',\n",
    "        'SchemaMapper1': 'SchemaMapper1',\n",
    "        'CompositeOutput1': 'Kafka__Composite_Output_Id',\n",
    "        'Debug': 'Debug'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx\n",
    "import datetime\n",
    "from streamsx.topology.context import ContextTypes, JobConfig\n",
    "from streamsx.topology import context\n",
    "\n",
    "def submit_app():\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    app = build_flow()\n",
    "\n",
    "    dt = datetime.datetime.now().strftime('%F_%T')\n",
    "    \n",
    "    job_config = JobConfig(job_name=f'{app.namespace}:{app.name}:{dt}', tracing='info')\n",
    "    job_config.add(cfg)\n",
    "\n",
    "    shutil.copytree('lib', 'python/modules/lib')\n",
    "    app.add_file_dependency('python', 'opt')\n",
    "\n",
    "    submission_result = streamsx.topology.context.submit(ContextTypes.DISTRIBUTED, app, config=cfg)\n",
    "    streams_job = submission_result.job\n",
    "    print(\"JobId: \", streams_job.id, \"\\nJob name: \", streams_job.name)\n",
    "submit_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the resource directory (Optional)\n",
    "Cleans up the resource folders used in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup()\n",
    "# import shutil\n",
    "# os.chdir(os.environ['PWD'])\n",
    "# if os.path.exists(resource_path):\n",
    "#     shutil.rmtree(resource_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
